# Thoth

Named after the Egyptian god of the moon, of reckoning, of learning, and of writing, is a **_tokenizer_**, purposed for interpreting text input for large-scale language models, such as OpenAI's GPT series of models (e.g., GPT-3.5, GPT-4, GitHub Copilot), Google's PaLM and Gemini, Meta's LLaMA family, and Anthropic's Claude models.

### Inspiration

I just felt like it. It seems like the bottleneck of LLMs at the moment, so it would be good to research and develop one, understand up-to-date architectures, and produce better open source implementations in hopes to provide AI at NLP.

### How to Use

Run each of the python files.

`python -u <directory of python file>`

### Architecture

Differs per tokenizer.

#### BasicTokenizer

- TBD

#### RegexTokenizer

- TBD

#### GPT4Tokenizer

- TBD

#### SentencePiece

- TBD

#### Llama2

- TBD

### Topics

- **Languages**: Python
- **Libraries/Frameworks/Tools**: Regex, Tiktoken, Unicodedata
- <ins>**Other**</ins>:
  - **Concepts**: Tokenization, Embeddings, LLMs, GPT2.0 & GPT4.0, Llama2, Sentencepiece, Byte-pair encoding (BPE), ...
